# LLM Module Specification Documents

> **T√†i li·ªáu ƒë·∫∑c t·∫£ ƒë·∫ßy ƒë·ªß cho module LLM c·ªßa PCM Desktop**

---

## üìö Danh M·ª•c T√†i Li·ªáu

### 1Ô∏è‚É£ [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md)

**Thi·∫øt k·∫ø ki·∫øn tr√∫c t·ªïng th·ªÉ cho LLM Module**

**N·ªôi dung ch√≠nh:**

- ‚úÖ Simplified API - M·ªôt ph∆∞∆°ng th·ª©c duy nh·∫•t `provider.chat()`
- ‚úÖ Provider Registry - Qu·∫£n l√Ω v√† chuy·ªÉn ƒë·ªïi providers
- ‚úÖ Token Limiting - Gi·ªõi h·∫°n tokens v·ªõi custom counter
- ‚úÖ System Messages - H·ªó tr·ª£ ƒë·∫ßy ƒë·ªß system messages
- ‚úÖ Standardized Function Calling - ƒê·ªãnh d·∫°ng chu·∫©n OpenAI
- ‚úÖ Function Registry - Qu·∫£n l√Ω t·∫≠p trung c√°c functions
- ‚úÖ Thinking Mode - H·ªó tr·ª£ reasoning models (o1, o3)
- ‚úÖ Event-Driven - Callbacks: onToken, onComplete, onError, onThinking, onToolCall
- ‚úÖ Provider Capabilities - Check support & list models
- ‚úÖ Common Patterns - Error handling, retry, token counting, context management

**Ki·∫øn tr√∫c:**

```
ProviderRegistry
  ‚îú‚îÄ‚îÄ OpenAI Provider
  ‚îú‚îÄ‚îÄ Anthropic Provider
  ‚îî‚îÄ‚îÄ Ollama Provider

Each Provider implements:
  ‚îú‚îÄ‚îÄ chat(messages, options)
  ‚îú‚îÄ‚îÄ configure(config)
  ‚îú‚îÄ‚îÄ getCapabilities()
  ‚îú‚îÄ‚îÄ getModels()
  ‚îî‚îÄ‚îÄ countTokens(text)
```

**ƒê·ªçc khi:**

- C·∫ßn hi·ªÉu t·ªïng quan v·ªÅ ki·∫øn tr√∫c LLM module
- Thi·∫øt k·∫ø providers m·ªõi
- Implement core features

---

### 2Ô∏è‚É£ [LLM_MULTIPLE_TOOLS_AND_SUMMARY.md](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md)

**Chi ti·∫øt v·ªÅ multiple tool calls v√† auto-summarization**

**N·ªôi dung ch√≠nh:**

- ‚úÖ **Multiple Tool Calls** - LLM c√≥ th·ªÉ g·ªçi nhi·ªÅu tools trong m·ªôt response
- ‚úÖ **Sequential Execution** - Th·ª±c thi tools theo th·ª© t·ª±
- ‚úÖ **Parallel Execution** - T·ªëi ∆∞u v·ªõi dependency analysis
- ‚úÖ **Auto Summarization** - T·ª± ƒë·ªông t√≥m t·∫Øt khi conversation d√†i
- ‚úÖ **Custom Summarizer** - T√πy ch·ªânh c√°ch t√≥m t·∫Øt
- ‚úÖ **Smart Context Management** - Qu·∫£n l√Ω context window th√¥ng minh

**Tool Execution Strategies:**

- Sequential: Th·ª±c thi t·ª´ng tool theo th·ª© t·ª±
- Parallel: Ph√¢n t√≠ch dependencies v√† ch·∫°y song song khi c√≥ th·ªÉ

**Summarization Strategies:**

- LLMSummarizer: D√πng LLM ƒë·ªÉ t√≥m t·∫Øt (ch·∫•t l∆∞·ª£ng cao)
- ExtractiveSummarizer: Tr√≠ch xu·∫•t key points (nhanh, r·∫ª)
- CustomSummarizer: T√πy ch·ªânh logic ri√™ng

**ƒê·ªçc khi:**

- Implement function calling v·ªõi nhi·ªÅu tools
- C·∫ßn qu·∫£n l√Ω conversations d√†i
- T·ªëi ∆∞u token usage

---

### 3Ô∏è‚É£ [LLM_FUNCTION_ANNOTATION_DESIGN.md](./LLM_FUNCTION_ANNOTATION_DESIGN.md)

**Annotation-based function definition v·ªõi auto-scanning**

**N·ªôi dung ch√≠nh:**

- ‚úÖ **@LLMFunction** - ƒê√°nh d·∫•u methods l√† LLM-callable
- ‚úÖ **@Param** - M√¥ t·∫£ parameters v·ªõi validation
- ‚úÖ **@FunctionProvider** - ƒê√°nh d·∫•u classes ch·ª©a functions
- ‚úÖ **Auto Scanning** - T·ª± ƒë·ªông qu√©t v√† ƒëƒÉng k√Ω functions
- ‚úÖ **DI Integration** - Th·ª±c thi trong application context
- ‚úÖ **Reflection-based Execution** - G·ªçi methods ƒë·ªông

**Example:**

```java
@FunctionProvider
public class SearchFunctions {
    
    @LLMFunction(
        name = "search_projects",
        description = "Search for projects in database"
    )
    public List<Project> searchProjects(
        @Param(description = "Search query", required = true)
        String query,
        
        @Param(description = "Maximum results", defaultValue = "10")
        int limit
    ) {
        // Implementation
        return projectService.search(query, limit);
    }
}
```

**Auto-registration:**

```java
// At startup
FunctionRegistry registry = FunctionRegistry.getInstance();
registry.scanPackage("com.noteflix.pcm.functions");
// All @LLMFunction methods are now registered!
```

**ƒê·ªçc khi:**

- Implement custom functions cho LLM
- Setup function scanning
- Integrate v·ªõi DI container

---

### 4Ô∏è‚É£ [LLM_LOGGING_DESIGN.md](./LLM_LOGGING_DESIGN.md)

**Logging v√† audit trail system cho LLM calls**

**N·ªôi dung ch√≠nh:**

- ‚úÖ **Complete Audit Trail** - L∆∞u ƒë·∫ßy ƒë·ªß request/response
- ‚úÖ **Tool Execution Logs** - Log t·ª´ng tool call v·ªõi params & results
- ‚úÖ **Multiple Storage** - Database (SQLite) ho·∫∑c File (JSON)
- ‚úÖ **Async Logging** - Kh√¥ng block LLM calls
- ‚úÖ **Queryable** - Search v√† filter logs d·ªÖ d√†ng
- ‚úÖ **Analytics** - Token usage, cost tracking, performance metrics

**Data Models:**

- **LLMCallLog**: To√†n b·ªô th√¥ng tin 1 LLM call
    - Request: messages, options, tools
    - Response: content, thinking, tool calls, usage
    - Metadata: timestamp, duration, user, session
    - Error: c√≥ l·ªói kh√¥ng, error message

- **ToolCallLog**: Chi ti·∫øt execution c·ªßa 1 tool
    - Input: tool name, arguments
    - Output: result ho·∫∑c error
    - Timing: execution duration

**Storage Options:**

- **DatabaseLogger**: SQLite v·ªõi indexes (fast queries)
- **FileLogger**: JSON files organized by date (simple)
- **CompositeLogger**: Log to multiple destinations

**ƒê·ªçc khi:**

- Setup logging cho production
- C·∫ßn tracking costs
- Debug LLM interactions
- Compliance requirements

---

### 5Ô∏è‚É£ [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md)

**Tool result caching v√† prompt template system**

**N·ªôi dung ch√≠nh:**

#### **A. Tool Result Caching**

- ‚úÖ **Cache Strategy Pattern** - Linh ho·∫°t ch·ªçn c√°ch x·ª≠ l√Ω tool results
- ‚úÖ **Smart Summarization** - T·ª± ƒë·ªông quy·∫øt ƒë·ªãnh full/summary
- ‚úÖ **Token Budget Control** - Ki·ªÉm so√°t chi ph√≠ tokens
- ‚úÖ **Adaptive Learning** - H·ªçc t·ª´ usage patterns

**Caching Strategies:**

1. **AlwaysFullStrategy** (Default)
    - Lu√¥n g·ª≠i full results
    - Accuracy t·ªëi ƒëa
    - Use case: Khi ch√≠nh x√°c quan tr·ªçng nh·∫•t

2. **SmartSummarizationStrategy**
    - T·ª± ƒë·ªông quy·∫øt ƒë·ªãnh d·ª±a tr√™n k√≠ch th∆∞·ªõc
    - Small results: full
    - Large results: summarize
    - Medium: depends on context window
    - Use case: C√¢n b·∫±ng accuracy vs cost

3. **TokenBudgetStrategy**
    - Strict token limit per tool result
    - Use case: Budget constraints

4. **AdaptiveStrategy**
    - H·ªçc t·ª´ history
    - Cache & reuse similar results
    - Avoid summarization n·∫øu g√¢y l·ªói
    - Use case: Production optimization

**Cache Decision Logic:**

```java
CacheDecision {
    shouldCache: boolean        // Cache ƒë·ªÉ reuse?
    shouldSummarize: boolean    // Summarize tr∆∞·ªõc khi g·ª≠i LLM?
    summarizationStrategy: String
    reason: String
}
```

#### **B. Prompt Template System**

- ‚úÖ **Template Registry** - Qu·∫£n l√Ω t·∫≠p trung prompts
- ‚úÖ **Variable Substitution** - Dynamic prompt generation
- ‚úÖ **i18n Support** - Multi-language prompts
- ‚úÖ **File-based Loading** - Load t·ª´ files
- ‚úÖ **Advanced Features** - Conditionals, loops, nested variables

**Built-in Templates:**

- `system.default` - Default system message
- `system.with_role` - System message v·ªõi role
- `summarize.conversation` - T√≥m t·∫Øt conversation
- `summarize.tool_result` - T√≥m t·∫Øt tool result
- `function.instruction` - Function calling instructions
- `thinking.instruction` - Thinking mode instructions
- `error.tool_execution` - Tool error recovery

**Multi-language Example:**

```java
promptRegistry.register("system.helpful", I18nPromptTemplate.builder()
    .templatesByLocale(Map.of(
        Locale.ENGLISH, "You are a helpful AI assistant.",
        Locale.forLanguageTag("vi"), "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch.",
        Locale.CHINESE, "‰Ω†ÊòØ‰∏Ä‰∏™ÊúâÁî®ÁöÑAIÂä©Êâã„ÄÇ"
    ))
    .build());

promptRegistry.setLocale(Locale.forLanguageTag("vi"));
String prompt = promptRegistry.render("system.helpful", Map.of());
// Output: "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch."
```

**ƒê·ªçc khi:**

- C·∫ßn t·ªëi ∆∞u token costs
- Setup prompt templates
- Implement multi-language support
- Cache tool results

---

### 6Ô∏è‚É£ [LLM_MODULE_STRUCTURE.md](./LLM_MODULE_STRUCTURE.md)

**Current structure c·ªßa LLM module (existing code)**

**N·ªôi dung ch√≠nh:**

- üìÇ Package structure hi·ªán t·∫°i
- üìÑ C√°c file quan tr·ªçng
- üîç Code c·∫ßn refactor

**ƒê·ªçc khi:**

- C·∫ßn hi·ªÉu code hi·ªán t·∫°i
- Planning refactoring
- Onboarding new developers

---

## üéØ Quick Reference

### **T√¥i mu·ªën...**

| M·ª•c ti√™u                     | ƒê·ªçc t√†i li·ªáu                                                             | Ph·∫ßn c·ª• th·ªÉ              |
|------------------------------|--------------------------------------------------------------------------|--------------------------|
| Hi·ªÉu t·ªïng quan ki·∫øn tr√∫c     | [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md)                       | ¬ß New Architecture       |
| Setup providers              | [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md)                       | ¬ß Provider Registry      |
| Implement function calling   | [LLM_FUNCTION_ANNOTATION_DESIGN.md](./LLM_FUNCTION_ANNOTATION_DESIGN.md) | To√†n b·ªô                  |
| Handle multiple tool calls   | [LLM_MULTIPLE_TOOLS_AND_SUMMARY.md](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md) | ¬ß Multiple Tool Calls    |
| Setup logging                | [LLM_LOGGING_DESIGN.md](./LLM_LOGGING_DESIGN.md)                         | ¬ß Complete Example       |
| T·ªëi ∆∞u token costs           | [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md)         | ¬ß Tool Result Caching    |
| Customize prompts            | [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md)         | ¬ß Prompt Template System |
| Multi-language prompts       | [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md)         | ¬ß Multi-language Support |
| Auto-summarize conversations | [LLM_MULTIPLE_TOOLS_AND_SUMMARY.md](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md) | ¬ß Auto Summarization     |

---

## üìä Feature Matrix

| Feature             | Design Doc                                    | Priority  | Status  |
|---------------------|-----------------------------------------------|-----------|---------|
| Provider Registry   | [Design](./LLM_REFACTOR_DESIGN.md)            | üî¥ High   | Pending |
| Token Counter       | [Design](./LLM_REFACTOR_DESIGN.md)            | üî¥ High   | Pending |
| Function Registry   | [Design](./LLM_REFACTOR_DESIGN.md)            | üî¥ High   | Pending |
| Event System        | [Design](./LLM_REFACTOR_DESIGN.md)            | üî¥ High   | Pending |
| Multiple Tool Calls | [Design](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md) | üü° Medium | Pending |
| Auto Summarization  | [Design](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md) | üü° Medium | Pending |
| Annotation Scanning | [Design](./LLM_FUNCTION_ANNOTATION_DESIGN.md) | üü° Medium | Pending |
| LLM Call Logging    | [Design](./LLM_LOGGING_DESIGN.md)             | üî¥ High   | Pending |
| Tool Result Caching | [Design](./LLM_TOOL_CACHE_AND_PROMPTS.md)     | üü° Medium | Pending |
| Prompt Templates    | [Design](./LLM_TOOL_CACHE_AND_PROMPTS.md)     | üü¢ Low    | Pending |

---

## üèóÔ∏è Implementation Order

### **Phase 1: Core Infrastructure** üî¥

1. Provider Interface & Registry
2. Token Counter (default + custom)
3. Message models v·ªõi System support
4. Event system (ChatEventListener)

**Docs:** [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md)

### **Phase 2: Function Calling** üî¥

1. Standardized Tool format
2. FunctionRegistry basic
3. Annotation scanning
4. DI integration

**Docs:
** [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md), [LLM_FUNCTION_ANNOTATION_DESIGN.md](./LLM_FUNCTION_ANNOTATION_DESIGN.md)

### **Phase 3: Providers** üî¥

1. Refactor OpenAI provider
2. Refactor Anthropic provider
3. Refactor Ollama provider
4. Provider capabilities

**Docs:** [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md)

### **Phase 4: Advanced Features** üü°

1. Multiple tool calls support
2. Tool result caching
3. Auto-summarization
4. Thinking mode

**Docs:
** [LLM_MULTIPLE_TOOLS_AND_SUMMARY.md](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md), [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md)

### **Phase 5: Observability** üî¥

1. LLM call logging
2. Tool execution logging
3. Analytics & statistics
4. UI integration

**Docs:** [LLM_LOGGING_DESIGN.md](./LLM_LOGGING_DESIGN.md)

### **Phase 6: Polish** üü¢

1. Prompt template system
2. i18n prompts
3. Error handling improvements
4. Documentation & examples

**Docs:** [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md)

---

## üîë Key Design Decisions

### **1. Why Provider Registry Pattern?**

- ‚úÖ Easy to switch between providers
- ‚úÖ Centralized configuration
- ‚úÖ Consistent API across providers
- ‚úÖ Support multiple active providers

### **2. Why Annotation-based Functions?**

- ‚úÖ Declarative & clean
- ‚úÖ Auto-discovery
- ‚úÖ Type-safe with validation
- ‚úÖ Easy to maintain

### **3. Why Cache Tool Results?**

- ‚úÖ Avoid redundant expensive operations
- ‚úÖ Control token costs
- ‚úÖ Improve response time
- ‚úÖ Flexible strategies per use case

### **4. Why Prompt Templates?**

- ‚úÖ Separate prompts from code
- ‚úÖ Easy A/B testing
- ‚úÖ Multi-language support
- ‚úÖ Version control friendly

### **5. Why Comprehensive Logging?**

- ‚úÖ Debugging LLM interactions
- ‚úÖ Cost tracking & optimization
- ‚úÖ Audit trail for compliance
- ‚úÖ Performance monitoring

---

## üìñ How to Read These Docs

### **For Architects/Tech Leads:**

1. Start with [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md) - Overall architecture
2. Review all ¬ß Architecture sections
3. Evaluate design decisions
4. Plan implementation phases

### **For Developers (Implementing):**

1. Read relevant spec for your feature
2. Follow code examples
3. Check ¬ß Integration sections
4. Refer to ¬ß File Structure for where to put code

### **For New Team Members:**

1. Start with [LLM_MODULE_STRUCTURE.md](./LLM_MODULE_STRUCTURE.md) - Current state
2. Read [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md) - Future state
3. Skim other docs to understand capabilities
4. Deep dive when implementing specific features

---

## üé® Design Principles

All specifications follow these principles:

1. **Simple by Default** - Easy to use for common cases
2. **Flexible for Advanced** - Powerful features available when needed
3. **Provider Agnostic** - Work with any LLM provider
4. **Type Safe** - Leverage Java type system
5. **Observable** - Log everything, measure everything
6. **Maintainable** - Clean architecture, clear separation
7. **Testable** - All components are unit testable

---

## üöÄ Getting Started

### **To Implement a New Feature:**

1. **Read the spec** - Find relevant document above
2. **Understand the design** - Review architecture diagrams
3. **Check dependencies** - See implementation order
4. **Write tests first** - Test-driven development
5. **Implement** - Follow code examples in specs
6. **Integrate** - Connect with existing components
7. **Document** - Update examples and README

### **To Propose Changes:**

1. **Identify the issue** - What's wrong with current design?
2. **Suggest solution** - How to improve?
3. **Update spec** - Modify relevant document
4. **Discuss trade-offs** - What are pros/cons?
5. **Get consensus** - Team agreement
6. **Implement** - Make it happen!

---

## üìû Questions?

**Architecture Questions:**

- Review [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md) first
- Check design principles section
- Discuss with tech lead

**Implementation Questions:**

- Find the relevant spec
- Check code examples
- Refer to ¬ß Integration sections

**Missing Information:**

- Document is incomplete
- Open an issue or discussion
- Propose additions to specs

---

## üìù Document Status

| Document                          | Version | Last Updated | Status     |
|-----------------------------------|---------|--------------|------------|
| LLM_REFACTOR_DESIGN.md            | 1.0     | 2025-11-12   | ‚úÖ Complete |
| LLM_MULTIPLE_TOOLS_AND_SUMMARY.md | 1.0     | 2025-11-12   | ‚úÖ Complete |
| LLM_FUNCTION_ANNOTATION_DESIGN.md | 1.0     | 2025-11-12   | ‚úÖ Complete |
| LLM_LOGGING_DESIGN.md             | 1.0     | 2025-11-12   | ‚úÖ Complete |
| LLM_TOOL_CACHE_AND_PROMPTS.md     | 1.0     | 2025-11-12   | ‚úÖ Complete |
| README.md (this)                  | 1.0     | 2025-11-12   | ‚úÖ Complete |

---

**All designs are complete and ready for implementation!** üéâ

Next step: Start Phase 1 implementation (Core Infrastructure)


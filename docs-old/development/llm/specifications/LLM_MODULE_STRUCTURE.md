# LLM Module Structure

## ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
src/main/java/com/noteflix/pcm/llm/
â”œâ”€â”€ ğŸ“¦ api/                          # Interfaces & Contracts
â”‚   â”œâ”€â”€ LLMClient.java              # Base interface cho táº¥t cáº£ LLM clients
â”‚   â”œâ”€â”€ StreamingCapable.java      # Interface cho streaming responses
â”‚   â”œâ”€â”€ FunctionCallingCapable.java # Interface cho function calling
â”‚   â””â”€â”€ EmbeddingsCapable.java     # Interface cho embeddings API
â”‚
â”œâ”€â”€ ğŸ”Œ client/                       # Provider Implementations
â”‚   â”œâ”€â”€ openai/
â”‚   â”‚   â”œâ”€â”€ OpenAIClient.java      # OpenAI GPT implementation
â”‚   â”‚   â””â”€â”€ SSEParser.java         # Server-Sent Events parser
â”‚   â”œâ”€â”€ anthropic/
â”‚   â”‚   â””â”€â”€ AnthropicClient.java   # Anthropic Claude implementation
â”‚   â””â”€â”€ ollama/
â”‚       â””â”€â”€ OllamaClient.java      # Ollama local models implementation
â”‚
â”œâ”€â”€ ğŸ­ factory/                      # Factory Pattern
â”‚   â””â”€â”€ LLMClientFactory.java      # Factory Ä‘á»ƒ táº¡o LLM clients
â”‚
â”œâ”€â”€ ğŸ“Š model/                        # Data Models
â”‚   â”œâ”€â”€ LLMRequest.java            # Request model
â”‚   â”œâ”€â”€ LLMResponse.java           # Response model
â”‚   â”œâ”€â”€ LLMChunk.java              # Streaming chunk model
â”‚   â”œâ”€â”€ Message.java               # Chat message model
â”‚   â”œâ”€â”€ StreamingObserver.java     # Observer pattern cho streaming
â”‚   â”œâ”€â”€ LLMProviderConfig.java     # Provider configuration
â”‚   â”œâ”€â”€ FunctionDefinition.java    # Function definition cho function calling
â”‚   â”œâ”€â”€ FunctionCall.java          # Function call result
â”‚   â”œâ”€â”€ EmbeddingsRequest.java     # Embeddings request
â”‚   â””â”€â”€ EmbeddingsResponse.java    # Embeddings response
â”‚
â”œâ”€â”€ ğŸ›¡ï¸ exception/                    # Custom Exceptions
â”‚   â”œâ”€â”€ LLMException.java          # Base exception
â”‚   â”œâ”€â”€ LLMProviderException.java  # Provider-specific exception
â”‚   â””â”€â”€ StreamingException.java    # Streaming-specific exception
â”‚
â”œâ”€â”€ âš™ï¸ middleware/                   # Middleware Components
â”‚   â”œâ”€â”€ RateLimiter.java           # Rate limiting
â”‚   â”œâ”€â”€ RetryPolicy.java           # Retry logic
â”‚   â””â”€â”€ RequestLogger.java         # Request/response logging
â”‚
â”œâ”€â”€ ğŸ¯ service/                      # High-level Services
â”‚   â””â”€â”€ LLMService.java            # Main service vá»›i business logic
â”‚
â””â”€â”€ ğŸ“š examples/                     # Code Examples
    â”œâ”€â”€ LLMUsageExample.java       # Basic usage examples
    â”œâ”€â”€ APIDemo.java               # API demo
    â””â”€â”€ MiddlewareExample.java     # Middleware usage examples
```

---

## ğŸ—ï¸ Architecture Pattern

### **Strategy Pattern** (Client Implementations)

```
LLMClient (Interface)
    â†‘
    â”œâ”€â”€ OpenAIClient
    â”œâ”€â”€ AnthropicClient
    â””â”€â”€ OllamaClient
```

### **Factory Pattern** (Client Creation)

```
LLMClientFactory
    â”‚
    â”œâ”€â”€ getClient(config) â†’ LLMClient
    â””â”€â”€ Cache clients by provider
```

### **Observer Pattern** (Streaming)

```
StreamingObserver (Interface)
    â”‚
    â”œâ”€â”€ onChunk(chunk)
    â”œâ”€â”€ onComplete()
    â””â”€â”€ onError(error)
```

---

## ğŸ“¦ Core Components

### 1ï¸âƒ£ **API Layer** (`api/`)

#### `LLMClient.java` - Base Interface

```java
public interface LLMClient {
    LLMResponse sendMessage(LLMRequest request);
    String getProviderName();
    boolean isAvailable();
    String getModelName();
}
```

**Má»¥c Ä‘Ã­ch:**

- âœ… Äá»‹nh nghÄ©a contract cho táº¥t cáº£ LLM providers
- âœ… Dependency Inversion Principle
- âœ… Dá»… dÃ ng swap providers

#### `StreamingCapable.java`

```java
public interface StreamingCapable {
    void streamMessage(LLMRequest request, StreamingObserver observer);
}
```

**Má»¥c Ä‘Ã­ch:**

- âœ… Support streaming responses (nhÆ° ChatGPT)
- âœ… Real-time token generation
- âœ… Better UX

#### `FunctionCallingCapable.java`

```java
public interface FunctionCallingCapable {
    LLMResponse sendMessageWithFunctions(
        LLMRequest request, 
        List<FunctionDefinition> functions
    );
}
```

**Má»¥c Ä‘Ã­ch:**

- âœ… Support OpenAI function calling
- âœ… Tool use (nhÆ° Anthropic)
- âœ… Agent capabilities

#### `EmbeddingsCapable.java`

```java
public interface EmbeddingsCapable {
    EmbeddingsResponse getEmbeddings(EmbeddingsRequest request);
}
```

**Má»¥c Ä‘Ã­ch:**

- âœ… Vector embeddings cho RAG
- âœ… Semantic search
- âœ… Knowledge base integration

---

### 2ï¸âƒ£ **Client Layer** (`client/`)

#### **OpenAI Client**

- File: `openai/OpenAIClient.java`
- Supports: GPT-4, GPT-3.5-turbo, GPT-4-turbo
- Features:
    - âœ… Chat completions
    - âœ… Streaming vá»›i SSE
    - âœ… Function calling
    - âœ… Embeddings (text-embedding-ada-002)

#### **Anthropic Client**

- File: `anthropic/AnthropicClient.java`
- Supports: Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Haiku
- Features:
    - âœ… Chat completions
    - âœ… Tool use
    - âœ… Long context (200k tokens)

#### **Ollama Client**

- File: `ollama/OllamaClient.java`
- Supports: Local models (Llama, Mistral, etc.)
- Features:
    - âœ… Local inference
    - âœ… No API keys needed
    - âœ… Privacy-first

---

### 3ï¸âƒ£ **Factory Layer** (`factory/`)

#### `LLMClientFactory.java`

```java
public class LLMClientFactory {
    public LLMClient getClient(LLMProviderConfig config) {
        return switch (config.getProvider()) {
            case OPENAI -> new OpenAIClient(config);
            case ANTHROPIC -> new AnthropicClient(config);
            case OLLAMA -> new OllamaClient(config);
        };
    }
}
```

**Features:**

- âœ… Singleton pattern
- âœ… Client caching
- âœ… Lazy initialization
- âœ… Easy to add new providers

---

### 4ï¸âƒ£ **Model Layer** (`model/`)

#### Key Models:

**`LLMRequest.java`**

```java
public class LLMRequest {
    private List<Message> messages;
    private String model;
    private double temperature;
    private int maxTokens;
    private Map<String, Object> additionalParams;
}
```

**`LLMResponse.java`**

```java
public class LLMResponse {
    private String content;
    private String model;
    private int tokensUsed;
    private long latencyMs;
    private String finishReason;
}
```

**`LLMChunk.java`** (for streaming)

```java
public class LLMChunk {
    private String content;
    private boolean isComplete;
    private String finishReason;
}
```

**`StreamingObserver.java`**

```java
public interface StreamingObserver {
    void onChunk(LLMChunk chunk);
    void onComplete();
    void onError(Throwable error);
}
```

**`LLMProviderConfig.java`**

```java
public class LLMProviderConfig {
    private LLMProvider provider;  // OPENAI, ANTHROPIC, OLLAMA
    private String apiKey;
    private String model;
    private String baseUrl;
    private int timeout;
}
```

---

### 5ï¸âƒ£ **Exception Layer** (`exception/`)

```
LLMException (Base)
    â†‘
    â”œâ”€â”€ LLMProviderException (Provider errors)
    â””â”€â”€ StreamingException (Streaming errors)
```

**Hierarchy:**

- âœ… Clear error types
- âœ… Easy to catch and handle
- âœ… Good error messages

---

### 6ï¸âƒ£ **Middleware Layer** (`middleware/`)

#### `RateLimiter.java`

```java
public class RateLimiter {
    // Token bucket algorithm
    public boolean tryAcquire();
    public void acquire() throws InterruptedException;
}
```

**Má»¥c Ä‘Ã­ch:**

- âœ… Prevent API rate limit errors
- âœ… Configurable rates
- âœ… Per-provider limits

#### `RetryPolicy.java`

```java
public class RetryPolicy {
    public <T> T execute(Callable<T> task);
    // Exponential backoff
    // Max retries
}
```

**Má»¥c Ä‘Ã­ch:**

- âœ… Handle transient errors
- âœ… Automatic retry
- âœ… Exponential backoff

#### `RequestLogger.java`

```java
public class RequestLogger {
    public void logRequest(LLMRequest request);
    public void logResponse(LLMResponse response);
}
```

**Má»¥c Ä‘Ã­ch:**

- âœ… Debug requests/responses
- âœ… Audit trail
- âœ… Performance monitoring

---

### 7ï¸âƒ£ **Service Layer** (`service/`)

#### `LLMService.java` - Main Service

```java
public class LLMService {
    // High-level methods
    public String chat(String message);
    public void chatStreaming(String message, StreamingObserver observer);
    public void switchProvider(LLMProviderConfig config);
    
    // Advanced
    public LLMResponse chatWithFunctions(String message, List<FunctionDefinition> functions);
    public double[] getEmbeddings(String text);
}
```

**Má»¥c Ä‘Ã­ch:**

- âœ… Simplified API
- âœ… Business logic
- âœ… Error handling
- âœ… Logging & metrics

---

## ğŸ¯ Usage Examples

### **Basic Chat**

```java
LLMService service = new LLMService();
service.initialize(LLMProviderConfig.builder()
    .provider(LLMProvider.OPENAI)
    .apiKey("sk-...")
    .model("gpt-4")
    .build());

String response = service.chat("Hello, how are you?");
System.out.println(response);
```

### **Streaming Chat**

```java
service.chatStreaming("Tell me a story", new StreamingObserver() {
    @Override
    public void onChunk(LLMChunk chunk) {
        System.out.print(chunk.getContent());
    }
    
    @Override
    public void onComplete() {
        System.out.println("\n[Done]");
    }
    
    @Override
    public void onError(Throwable error) {
        System.err.println("Error: " + error.getMessage());
    }
});
```

### **Function Calling**

```java
List<FunctionDefinition> functions = List.of(
    FunctionDefinition.builder()
        .name("get_weather")
        .description("Get current weather")
        .parameters(...)
        .build()
);

LLMResponse response = service.chatWithFunctions(
    "What's the weather in Tokyo?", 
    functions
);

if (response.hasFunctionCall()) {
    FunctionCall call = response.getFunctionCall();
    // Execute function...
}
```

### **Switch Providers**

```java
// Start with OpenAI
service.initialize(openAIConfig);

// Switch to Anthropic
service.switchProvider(anthropicConfig);

// Switch to local Ollama
service.switchProvider(ollamaConfig);
```

---

## âœ… Design Principles Applied

### **SOLID Principles**

1. âœ… **Single Responsibility**
    - Each client handles one provider
    - Each middleware handles one concern
    - Clear separation

2. âœ… **Open/Closed**
    - Easy to add new providers (extend)
    - Don't need to modify existing code

3. âœ… **Liskov Substitution**
    - All clients can replace `LLMClient`
    - Polymorphism works correctly

4. âœ… **Interface Segregation**
    - Small, focused interfaces
    - Optional capabilities (Streaming, Functions, Embeddings)

5. âœ… **Dependency Inversion**
    - Depend on abstractions (`LLMClient`)
    - Not on concrete implementations

### **Design Patterns**

- âœ… **Strategy Pattern** - Multiple LLM providers
- âœ… **Factory Pattern** - Client creation
- âœ… **Observer Pattern** - Streaming
- âœ… **Singleton Pattern** - Factory instance
- âœ… **Builder Pattern** - Config objects
- âœ… **Middleware Pattern** - Request/response processing

---

## ğŸš€ Benefits

### **Flexibility**

- âœ… Easy to switch providers
- âœ… Support multiple providers simultaneously
- âœ… Provider-specific features available

### **Maintainability**

- âœ… Clear structure
- âœ… Easy to find code
- âœ… Well-documented

### **Testability**

- âœ… Mock `LLMClient` interface
- âœ… Test each provider independently
- âœ… Integration tests with real APIs

### **Extensibility**

- âœ… Add new providers easily
- âœ… Add new capabilities
- âœ… Plugin architecture ready

### **Performance**

- âœ… Client caching
- âœ… Connection pooling
- âœ… Rate limiting
- âœ… Retry logic

---

## ğŸ“ˆ Future Enhancements

### Planned Features:

- [ ] **Embeddings Support** - For RAG systems
- [ ] **Vision APIs** - GPT-4 Vision, Claude with images
- [ ] **Audio APIs** - Whisper, TTS
- [ ] **Custom Providers** - Easy plugin system
- [ ] **Prompt Templates** - Reusable prompt management
- [ ] **Token Counting** - Pre-request token estimation
- [ ] **Cost Tracking** - Monitor API costs
- [ ] **Caching Layer** - Cache similar requests
- [ ] **A/B Testing** - Compare providers/models
- [ ] **Load Balancing** - Distribute across providers

---

## ğŸ“š Related Documentation

- [AI Assistant Architecture](./development/ai-assistant/AI_ASSISTANT_REFACTOR_PLAN.md)
- [MVVM Refactoring](../MVVM_REFACTORING_COMPLETE.md)
- [Best Practices](../BESTPRACTICES.md)

---

**Created:** November 12, 2025  
**Version:** 1.0.0  
**Status:** âœ… Production Ready


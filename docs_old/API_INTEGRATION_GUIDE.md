# H∆∞·ªõng D·∫´n T√≠ch H·ª£p API - PCM Desktop

## üìã T·ªïng Quan

PCM Desktop cung c·∫•p h·ªá th·ªëng t√≠ch h·ª£p API m·∫°nh m·∫Ω v√† linh ho·∫°t ƒë·ªÉ g·ªçi c√°c d·ªãch v·ª• LLM (Large Language Model) kh√°c nhau nh∆∞ OpenAI GPT, Anthropic Claude, v√† Ollama. H·ªá th·ªëng ƒë∆∞·ª£c thi·∫øt k·∫ø theo m√¥ h√¨nh ki·∫øn tr√∫c clean architecture v·ªõi kh·∫£ nƒÉng m·ªü r·ªông v√† d·ªÖ b·∫£o tr√¨.

## üèóÔ∏è Ki·∫øn Tr√∫c H·ªá Th·ªëng

```
üìÅ com.noteflix.pcm.llm/
‚îú‚îÄ‚îÄ üìÇ api/                     # Interfaces v√† contracts
‚îÇ   ‚îú‚îÄ‚îÄ LLMClient.java         # Interface c∆° b·∫£n cho client
‚îÇ   ‚îú‚îÄ‚îÄ StreamingCapable.java  # Interface cho streaming
‚îÇ   ‚îú‚îÄ‚îÄ FunctionCallingCapable.java # Interface cho function calling
‚îÇ   ‚îî‚îÄ‚îÄ EmbeddingsCapable.java # Interface cho embeddings
‚îú‚îÄ‚îÄ üìÇ client/                  # Implementations cho t·ª´ng provider
‚îÇ   ‚îú‚îÄ‚îÄ openai/OpenAIClient.java
‚îÇ   ‚îú‚îÄ‚îÄ anthropic/AnthropicClient.java
‚îÇ   ‚îî‚îÄ‚îÄ ollama/OllamaClient.java
‚îú‚îÄ‚îÄ üìÇ service/                # High-level services
‚îÇ   ‚îî‚îÄ‚îÄ LLMService.java        # Service ch√≠nh
‚îú‚îÄ‚îÄ üìÇ model/                  # Data models
‚îÇ   ‚îú‚îÄ‚îÄ LLMRequest.java
‚îÇ   ‚îú‚îÄ‚îÄ LLMResponse.java
‚îÇ   ‚îú‚îÄ‚îÄ Message.java
‚îÇ   ‚îî‚îÄ‚îÄ LLMProviderConfig.java
‚îî‚îÄ‚îÄ üìÇ factory/               # Factory pattern
    ‚îî‚îÄ‚îÄ LLMClientFactory.java
```

## üöÄ C√°ch S·ª≠ D·ª•ng

### 1. C·∫•u H√¨nh Provider

#### OpenAI Configuration

```java
LLMProviderConfig openaiConfig = LLMProviderConfig.builder()
    .provider(LLMProviderConfig.Provider.OPENAI)
    .name("OpenAI GPT-4")
    .url("https://api.openai.com/v1/chat/completions")
    .token(System.getenv("OPENAI_API_KEY"))  // ƒê·∫∑t trong environment variable
    .model("gpt-4")
    .supportsStreaming(true)
    .supportsFunctionCalling(true)
    .timeout(30)
    .maxRetries(3)
    .build();
```

#### Anthropic Claude Configuration

```java
LLMProviderConfig claudeConfig = LLMProviderConfig.builder()
    .provider(LLMProviderConfig.Provider.ANTHROPIC)
    .name("Claude 3.5 Sonnet")
    .url("https://api.anthropic.com/v1/messages")
    .token(System.getenv("ANTHROPIC_API_KEY"))
    .model("claude-3-5-sonnet-20241022")
    .headers(Map.of("anthropic-version", "2023-06-01"))
    .supportsStreaming(true)
    .supportsFunctionCalling(true)
    .timeout(45)
    .build();
```

#### Ollama Local Configuration

```java
LLMProviderConfig ollamaConfig = LLMProviderConfig.builder()
    .provider(LLMProviderConfig.Provider.OLLAMA)
    .name("Local Llama 3")
    .url("http://localhost:11434/api/chat")
    .model("llama3")
    .supportsStreaming(true)
    .supportsFunctionCalling(false)
    .timeout(60)
    .build();
```

### 2. Kh·ªüi T·∫°o Service

```java
// T·∫°o v√† kh·ªüi t·∫°o LLMService
LLMService llmService = new LLMService();
llmService.initialize(openaiConfig);
```

### 3. Chat ƒê∆°n Gi·∫£n

```java
public class SimpleChatExample {
    public void basicChat() {
        try {
            // G·ª≠i tin nh·∫Øn ƒë∆°n gi·∫£n
            String response = llmService.chat("H√£y gi·∫£i th√≠ch v·ªÅ Java Streams");
            System.out.println("AI Response: " + response);
            
        } catch (Exception e) {
            log.error("L·ªói khi g·ªçi API: {}", e.getMessage());
        }
    }
}
```

### 4. Chat V·ªõi C·∫•u H√¨nh Chi Ti·∫øt

```java
public class DetailedChatExample {
    public void detailedChat() {
        // T·∫°o request v·ªõi c·∫•u h√¨nh chi ti·∫øt
        LLMRequest request = LLMRequest.builder()
            .model("gpt-4")
            .messages(List.of(
                Message.system("B·∫°n l√† m·ªôt chuy√™n gia Java programming."),
                Message.user("H√£y vi·∫øt v√≠ d·ª• v·ªÅ Singleton pattern v·ªõi thread-safety")
            ))
            .temperature(0.7)        // ƒê·ªô s√°ng t·∫°o (0-2)
            .maxTokens(1000)        // Gi·ªõi h·∫°n s·ªë token response
            .topP(0.9)              // Nucleus sampling
            .frequencyPenalty(0.0)  // Penalty cho t·ª´ l·∫∑p l·∫°i
            .presencePenalty(0.0)   // Penalty cho ch·ªß ƒë·ªÅ l·∫∑p l·∫°i
            .build();
        
        try {
            LLMResponse response = llmService.sendMessage(request);
            
            // Truy c·∫≠p th√¥ng tin chi ti·∫øt
            System.out.println("Content: " + response.getContent());
            System.out.println("Model: " + response.getModel());
            System.out.println("Tokens Used: " + response.getUsage().getTotalTokens());
            System.out.println("Finish Reason: " + response.getFinishReason());
            
        } catch (Exception e) {
            log.error("L·ªói API call: {}", e.getMessage());
        }
    }
}
```

### 5. Cu·ªôc Tr√≤ Chuy·ªán Nhi·ªÅu L∆∞·ª£t

```java
public class ConversationExample {
    public void multiTurnConversation() {
        // S·ª≠ d·ª•ng ConversationBuilder ƒë·ªÉ qu·∫£n l√Ω ng·ªØ c·∫£nh
        LLMService.ConversationBuilder conversation = llmService.newConversation()
            .addSystemMessage("B·∫°n l√† tr·ª£ l√Ω l·∫≠p tr√¨nh Java chuy√™n nghi·ªáp.")
            .addUserMessage("T√¥i c·∫ßn t·∫°o m·ªôt REST API v·ªõi Spring Boot")
            .temperature(0.8)
            .maxTokens(800);
        
        try {
            // L∆∞·ª£t 1
            LLMResponse response1 = conversation.send();
            System.out.println("AI: " + response1.getContent());
            
            // L∆∞·ª£t 2 - ti·∫øp t·ª•c cu·ªôc tr√≤ chuy·ªán
            conversation.addUserMessage("L√†m sao ƒë·ªÉ th√™m authentication v·ªõi JWT?");
            LLMResponse response2 = conversation.send();
            System.out.println("AI: " + response2.getContent());
            
            // L∆∞·ª£t 3
            conversation.addUserMessage("C√≥ th·ªÉ show code example kh√¥ng?");
            LLMResponse response3 = conversation.send();
            System.out.println("AI: " + response3.getContent());
            
            // Xem to√†n b·ªô l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán
            List<Message> history = conversation.getMessages();
            System.out.println("Conversation has " + history.size() + " messages");
            
        } catch (Exception e) {
            log.error("L·ªói conversation: {}", e.getMessage());
        }
    }
}
```

### 6. Streaming Response (Ph·∫£n H·ªìi Theo Th·ªùi Gian Th·ª±c)

```java
public class StreamingExample {
    public void streamingChat() {
        LLMRequest streamRequest = LLMRequest.builder()
            .model("gpt-3.5-turbo")
            .messages(List.of(Message.user("Vi·∫øt m·ªôt c√¢u chuy·ªán ng·∫Øn v·ªÅ AI")))
            .stream(true)
            .build();
        
        // C√°ch 1: S·ª≠ d·ª•ng Observer Pattern
        llmService.streamMessage(streamRequest, new StreamingObserver() {
            @Override
            public void onChunk(LLMChunk chunk) {
                // Hi·ªÉn th·ªã t·ª´ng chunk khi nh·∫≠n ƒë∆∞·ª£c
                System.out.print(chunk.getContent());
                System.out.flush();
            }
            
            @Override
            public void onComplete() {
                System.out.println("\\n[Stream completed]");
            }
            
            @Override
            public void onError(Throwable error) {
                System.err.println("Stream error: " + error.getMessage());
            }
        });
        
        // C√°ch 2: S·ª≠ d·ª•ng Java Stream API
        try {
            Stream<LLMChunk> stream = llmService.streamMessage(streamRequest);
            stream.forEach(chunk -> {
                System.out.print(chunk.getContent());
                System.out.flush();
            });
        } catch (Exception e) {
            log.error("Streaming error: {}", e.getMessage());
        }
    }
}
```

### 7. Function Calling (G·ªçi H√†m)

```java
public class FunctionCallingExample {
    public void functionCallingDemo() {
        // ƒê·ªãnh nghƒ©a function
        FunctionDefinition weatherFunction = FunctionDefinition.builder()
            .name("get_weather")
            .description("L·∫•y th√¥ng tin th·ªùi ti·∫øt hi·ªán t·∫°i c·ªßa m·ªôt th√†nh ph·ªë")
            .parameters(Map.of(
                "type", "object",
                "properties", Map.of(
                    "city", Map.of(
                        "type", "string",
                        "description", "T√™n th√†nh ph·ªë, v√≠ d·ª•: 'H√† N·ªôi', 'TP.HCM'"
                    ),
                    "unit", Map.of(
                        "type", "string",
                        "enum", List.of("celsius", "fahrenheit"),
                        "description", "ƒê∆°n v·ªã nhi·ªát ƒë·ªô"
                    )
                ),
                "required", List.of("city")
            ))
            .build();
        
        LLMRequest request = LLMRequest.builder()
            .model("gpt-3.5-turbo")
            .messages(List.of(
                Message.user("Th·ªùi ti·∫øt ·ªü H√† N·ªôi h√¥m nay nh∆∞ th·∫ø n√†o?")
            ))
            .build();
        
        try {
            LLMResponse response = llmService.sendWithFunctions(
                request, 
                List.of(weatherFunction)
            );
            
            if (response.hasFunctionCall()) {
                FunctionCall call = response.getFunctionCall();
                System.out.println("AI mu·ªën g·ªçi function: " + call.getName());
                System.out.println("V·ªõi tham s·ªë: " + call.getArguments());
                
                // Th·ª±c hi·ªán function call th·∫≠t
                String weatherResult = callWeatherAPI(call.getArguments());
                
                // G·ª≠i k·∫øt qu·∫£ l·∫°i cho AI
                LLMRequest followUpRequest = LLMRequest.builder()
                    .model("gpt-3.5-turbo")
                    .messages(List.of(
                        Message.user("Th·ªùi ti·∫øt ·ªü H√† N·ªôi h√¥m nay nh∆∞ th·∫ø n√†o?"),
                        Message.assistant("", call), // Message v·ªõi function call
                        Message.function(call.getName(), weatherResult)
                    ))
                    .build();
                
                LLMResponse finalResponse = llmService.sendMessage(followUpRequest);
                System.out.println("K·∫øt qu·∫£ cu·ªëi: " + finalResponse.getContent());
                
            } else {
                System.out.println("AI Response: " + response.getContent());
            }
            
        } catch (Exception e) {
            log.error("Function calling error: {}", e.getMessage());
        }
    }
    
    private String callWeatherAPI(String arguments) {
        // Mock implementation - trong th·ª±c t·∫ø s·∫Ω g·ªçi API th·ªùi ti·∫øt th·∫≠t
        return "{\\"temperature\\": 28, \\"condition\\": \\"sunny\\", \\"humidity\\": 65}";
    }
}
```

### 8. Chuy·ªÉn ƒê·ªïi Provider

```java
public class ProviderSwitchingExample {
    public void switchProviders() {
        LLMService service = new LLMService();
        
        // B·∫Øt ƒë·∫ßu v·ªõi OpenAI
        LLMProviderConfig openaiConfig = LLMProviderConfig.builder()
            .provider(LLMProviderConfig.Provider.OPENAI)
            .url("https://api.openai.com/v1/chat/completions")
            .token(System.getenv("OPENAI_API_KEY"))
            .model("gpt-3.5-turbo")
            .build();
        
        service.initialize(openaiConfig);
        String response1 = service.chat("Hello from OpenAI!");
        System.out.println("OpenAI: " + response1);
        
        // Chuy·ªÉn sang Anthropic
        LLMProviderConfig claudeConfig = LLMProviderConfig.builder()
            .provider(LLMProviderConfig.Provider.ANTHROPIC)
            .url("https://api.anthropic.com/v1/messages")
            .token(System.getenv("ANTHROPIC_API_KEY"))
            .model("claude-3-5-sonnet-20241022")
            .build();
        
        service.switchProvider(claudeConfig);
        String response2 = service.chat("Hello from Claude!");
        System.out.println("Claude: " + response2);
        
        // Ki·ªÉm tra kh·∫£ nƒÉng c·ªßa provider hi·ªán t·∫°i
        System.out.println("Current provider: " + service.getCurrentProvider());
        System.out.println("Supports streaming: " + service.supportsStreaming());
        System.out.println("Supports function calling: " + service.supportsFunctionCalling());
    }
}
```

## üîß Configuration Management

### 1. Environment Variables

T·∫°o file `.env` ho·∫∑c ƒë·∫∑t trong system environment:

```bash
# OpenAI
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_MODEL=gpt-4

# Anthropic
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Custom endpoints
CUSTOM_LLM_URL=https://your-custom-api.com/v1/chat
CUSTOM_LLM_TOKEN=your-custom-token
```

### 2. Configuration File

T·∫°o file `llm-config.json`:

```json
{
  "providers": [
    {
      "name": "OpenAI GPT-4",
      "provider": "OPENAI",
      "url": "https://api.openai.com/v1/chat/completions",
      "model": "gpt-4",
      "supportsStreaming": true,
      "supportsFunctionCalling": true,
      "timeout": 30,
      "maxRetries": 3
    },
    {
      "name": "Claude 3.5 Sonnet", 
      "provider": "ANTHROPIC",
      "url": "https://api.anthropic.com/v1/messages",
      "model": "claude-3-5-sonnet-20241022",
      "headers": {
        "anthropic-version": "2023-06-01"
      },
      "supportsStreaming": true,
      "supportsFunctionCalling": true,
      "timeout": 45
    },
    {
      "name": "Local Ollama",
      "provider": "OLLAMA", 
      "url": "http://localhost:11434/api/chat",
      "model": "llama3",
      "supportsStreaming": true,
      "supportsFunctionCalling": false,
      "timeout": 60
    }
  ]
}
```

### 3. Configuration Loader

```java
public class ConfigurationManager {
    private static final String CONFIG_FILE = "llm-config.json";
    
    public static List<LLMProviderConfig> loadConfigurations() {
        try {
            ObjectMapper mapper = new ObjectMapper();
            JsonNode root = mapper.readTree(new File(CONFIG_FILE));
            JsonNode providers = root.get("providers");
            
            List<LLMProviderConfig> configs = new ArrayList<>();
            for (JsonNode provider : providers) {
                LLMProviderConfig config = LLMProviderConfig.builder()
                    .provider(LLMProviderConfig.Provider.valueOf(
                        provider.get("provider").asText()
                    ))
                    .name(provider.get("name").asText())
                    .url(provider.get("url").asText())
                    .token(getTokenFromEnv(provider.get("provider").asText()))
                    .model(provider.get("model").asText())
                    .supportsStreaming(provider.get("supportsStreaming").asBoolean())
                    .supportsFunctionCalling(provider.get("supportsFunctionCalling").asBoolean())
                    .timeout(provider.get("timeout").asInt())
                    .maxRetries(provider.path("maxRetries").asInt(3))
                    .build();
                
                configs.add(config);
            }
            
            return configs;
            
        } catch (Exception e) {
            log.error("Error loading configuration: {}", e.getMessage());
            return Collections.emptyList();
        }
    }
    
    private static String getTokenFromEnv(String provider) {
        switch (provider) {
            case "OPENAI": return System.getenv("OPENAI_API_KEY");
            case "ANTHROPIC": return System.getenv("ANTHROPIC_API_KEY");
            case "OLLAMA": return ""; // No token needed
            default: return System.getenv("CUSTOM_LLM_TOKEN");
        }
    }
}
```

## üîí X·ª≠ L√Ω L·ªói v√† B·∫£o M·∫≠t

### 1. Exception Handling

```java
public class ErrorHandlingExample {
    public void robustAPICall() {
        try {
            String response = llmService.chat("Test message");
            System.out.println("Success: " + response);
            
        } catch (LLMProviderException e) {
            // L·ªói t·ª´ provider (API key sai, model kh√¥ng t·ªìn t·∫°i, v.v.)
            log.error("Provider error: {}", e.getMessage());
            System.err.println("L·ªói t·ª´ nh√† cung c·∫•p: " + e.getMessage());
            
        } catch (StreamingException e) {
            // L·ªói trong qu√° tr√¨nh streaming
            log.error("Streaming error: {}", e.getMessage());
            System.err.println("L·ªói streaming: " + e.getMessage());
            
        } catch (LLMException e) {
            // L·ªói chung t·ª´ h·ªá th·ªëng LLM
            log.error("LLM error: {}", e.getMessage());
            System.err.println("L·ªói h·ªá th·ªëng LLM: " + e.getMessage());
            
        } catch (Exception e) {
            // L·ªói kh√¥ng mong ƒë·ª£i kh√°c
            log.error("Unexpected error: {}", e.getMessage());
            System.err.println("L·ªói kh√¥ng x√°c ƒë·ªãnh: " + e.getMessage());
        }
    }
}
```

### 2. Rate Limiting

```java
public class RateLimitingExample {
    public void implementRateLimit() {
        // S·ª≠ d·ª•ng middleware cho rate limiting
        LLMService service = new LLMService();
        
        // Gi·ªõi h·∫°n 10 requests/minute
        RateLimiter rateLimiter = new RateLimiter(10, Duration.ofMinutes(1));
        
        for (int i = 0; i < 15; i++) {
            try {
                if (rateLimiter.allowRequest()) {
                    String response = service.chat("Message " + i);
                    System.out.println("Response " + i + ": " + response);
                } else {
                    System.out.println("Rate limit exceeded, waiting...");
                    Thread.sleep(1000);
                    i--; // Retry
                }
            } catch (Exception e) {
                log.error("Error in request {}: {}", i, e.getMessage());
            }
        }
    }
}
```

### 3. Retry Logic

```java
public class RetryExample {
    public void implementRetry() {
        RetryPolicy retryPolicy = new RetryPolicy(
            3,              // max attempts  
            Duration.ofSeconds(1),  // initial delay
            2.0             // backoff multiplier
        );
        
        try {
            String response = retryPolicy.execute(() -> {
                return llmService.chat("Important message that must succeed");
            });
            
            System.out.println("Success after retries: " + response);
            
        } catch (Exception e) {
            log.error("Failed after all retry attempts: {}", e.getMessage());
        }
    }
}
```

## üìä Monitoring v√† Logging

### 1. Request Logging

```java
public class RequestLoggingExample {
    public void enableRequestLogging() {
        // Enable request/response logging
        RequestLogger logger = new RequestLogger();
        
        LLMService service = new LLMService();
        service.addMiddleware(logger);
        
        // All requests will now be logged
        String response = service.chat("Test message");
        
        // Log output:
        // [REQUEST] POST https://api.openai.com/v1/chat/completions
        // [REQUEST BODY] {"model":"gpt-3.5-turbo","messages":[...]}
        // [RESPONSE] 200 OK (1.2s)
        // [RESPONSE BODY] {"choices":[...]}
    }
}
```

### 2. Metrics Collection

```java
public class MetricsExample {
    private final MeterRegistry meterRegistry = Metrics.globalRegistry;
    
    public void collectMetrics() {
        // Counter cho s·ªë l∆∞·ª£ng requests
        Counter requestCounter = Counter.builder("llm.requests.total")
            .description("Total number of LLM requests")
            .tag("provider", "openai")
            .register(meterRegistry);
        
        // Timer cho response time
        Timer responseTimer = Timer.builder("llm.response.time")
            .description("LLM response time")
            .register(meterRegistry);
        
        // Gauge cho s·ªë tokens s·ª≠ d·ª•ng
        AtomicInteger tokensUsed = new AtomicInteger(0);
        Gauge.builder("llm.tokens.used")
            .description("Total tokens used")
            .register(meterRegistry, tokensUsed, AtomicInteger::get);
        
        // S·ª≠ d·ª•ng metrics
        Timer.Sample sample = Timer.start(meterRegistry);
        try {
            LLMResponse response = llmService.sendMessage(request);
            requestCounter.increment();
            tokensUsed.addAndGet(response.getUsage().getTotalTokens());
        } finally {
            sample.stop(responseTimer);
        }
    }
}
```

## üß™ Testing

### 1. Unit Tests

```java
@TestMethodOrder(OrderAnnotation.class)
public class LLMServiceTest {
    
    @Mock
    private LLMClient mockClient;
    
    @InjectMocks 
    private LLMService llmService;
    
    @BeforeEach
    void setUp() {
        MockitoAnnotations.openMocks(this);
    }
    
    @Test
    @Order(1)
    void testBasicChat() {
        // Given
        String userMessage = "Hello";
        String expectedResponse = "Hi there!";
        
        LLMResponse mockResponse = LLMResponse.builder()
            .content(expectedResponse)
            .model("gpt-3.5-turbo")
            .build();
            
        when(mockClient.sendMessage(any(LLMRequest.class)))
            .thenReturn(mockResponse);
        
        // When
        String actualResponse = llmService.chat(userMessage);
        
        // Then
        assertEquals(expectedResponse, actualResponse);
        verify(mockClient, times(1)).sendMessage(any(LLMRequest.class));
    }
    
    @Test
    @Order(2)
    void testStreamingSupport() {
        // Given
        StreamingCapable streamingClient = mock(StreamingCapable.class);
        LLMChunk chunk1 = LLMChunk.builder().content("Hello").build();
        LLMChunk chunk2 = LLMChunk.builder().content(" World").build();
        
        when(streamingClient.streamMessage(any(LLMRequest.class)))
            .thenReturn(Stream.of(chunk1, chunk2));
        
        // When
        Stream<LLMChunk> stream = ((StreamingCapable) streamingClient)
            .streamMessage(LLMRequest.builder().build());
        
        // Then
        String result = stream
            .map(LLMChunk::getContent)
            .collect(Collectors.joining());
        assertEquals("Hello World", result);
    }
}
```

### 2. Integration Tests

```java
@SpringBootTest
@TestPropertySource(properties = {
    "llm.openai.api-key=test-key",
    "llm.openai.model=gpt-3.5-turbo"
})
public class LLMIntegrationTest {
    
    @Autowired
    private LLMService llmService;
    
    @Test
    @EnabledIf("#{environment['OPENAI_API_KEY'] != null}")
    void testRealOpenAIIntegration() {
        // Ch·ªâ ch·∫°y khi c√≥ API key th·∫≠t
        LLMProviderConfig config = LLMProviderConfig.builder()
            .provider(LLMProviderConfig.Provider.OPENAI)
            .url("https://api.openai.com/v1/chat/completions")
            .token(System.getenv("OPENAI_API_KEY"))
            .model("gpt-3.5-turbo")
            .build();
        
        llmService.initialize(config);
        
        String response = llmService.chat("Say 'Integration test successful'");
        
        assertNotNull(response);
        assertFalse(response.isEmpty());
        assertTrue(response.toLowerCase().contains("integration"));
    }
}
```

## üìù Best Practices

### 1. API Key Security
- ‚úÖ Lu√¥n l∆∞u API keys trong environment variables
- ‚úÖ Kh√¥ng commit API keys v√†o code repository  
- ‚úÖ S·ª≠ d·ª•ng secret management tools trong production
- ‚úÖ Rotate API keys ƒë·ªãnh k·ª≥

### 2. Error Handling
- ‚úÖ Implement retry logic v·ªõi exponential backoff
- ‚úÖ Set reasonable timeouts
- ‚úÖ Log errors v·ªõi ƒë·ªß context ƒë·ªÉ debug
- ‚úÖ C√≥ fallback mechanisms khi API kh√¥ng kh·∫£ d·ª•ng

### 3. Performance
- ‚úÖ Cache responses khi c√≥ th·ªÉ
- ‚úÖ Implement connection pooling
- ‚úÖ Monitor API usage v√† costs
- ‚úÖ S·ª≠ d·ª•ng streaming cho responses d√†i

### 4. Cost Optimization
- ‚úÖ Monitor token usage
- ‚úÖ S·ª≠ d·ª•ng model ph√π h·ª£p cho t·ª´ng use case
- ‚úÖ Implement usage limits
- ‚úÖ Cache expensive operations

## üîó T√†i Li·ªáu Tham Kh·∫£o

- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [Anthropic API Documentation](https://docs.anthropic.com/claude/reference)
- [Ollama API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)

## üìû H·ªó Tr·ª£

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ khi s·ª≠ d·ª•ng API integration:

1. Ki·ªÉm tra logs trong `logs/pcm-desktop.log`
2. Verify API keys v√† configuration
3. Test connectivity t·ªõi API endpoints
4. Tham kh·∫£o code examples trong `com.noteflix.pcm.llm.examples`

---

*T√†i li·ªáu n√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n. Vui l√≤ng check phi√™n b·∫£n m·ªõi nh·∫•t tr√™n repository.*
# LLM Module Specification Documents

> **TÃ i liá»‡u Ä‘áº·c táº£ Ä‘áº§y Ä‘á»§ cho module LLM cá»§a PCM Desktop**

---

## ğŸ“š Danh Má»¥c TÃ i Liá»‡u

### 1ï¸âƒ£ [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md)
**Thiáº¿t káº¿ kiáº¿n trÃºc tá»•ng thá»ƒ cho LLM Module**

**Ná»™i dung chÃ­nh:**
- âœ… Simplified API - Má»™t phÆ°Æ¡ng thá»©c duy nháº¥t `provider.chat()`
- âœ… Provider Registry - Quáº£n lÃ½ vÃ  chuyá»ƒn Ä‘á»•i providers
- âœ… Token Limiting - Giá»›i háº¡n tokens vá»›i custom counter
- âœ… System Messages - Há»— trá»£ Ä‘áº§y Ä‘á»§ system messages
- âœ… Standardized Function Calling - Äá»‹nh dáº¡ng chuáº©n OpenAI
- âœ… Function Registry - Quáº£n lÃ½ táº­p trung cÃ¡c functions
- âœ… Thinking Mode - Há»— trá»£ reasoning models (o1, o3)
- âœ… Event-Driven - Callbacks: onToken, onComplete, onError, onThinking, onToolCall
- âœ… Provider Capabilities - Check support & list models
- âœ… Common Patterns - Error handling, retry, token counting, context management

**Kiáº¿n trÃºc:**
```
ProviderRegistry
  â”œâ”€â”€ OpenAI Provider
  â”œâ”€â”€ Anthropic Provider
  â””â”€â”€ Ollama Provider

Each Provider implements:
  â”œâ”€â”€ chat(messages, options)
  â”œâ”€â”€ configure(config)
  â”œâ”€â”€ getCapabilities()
  â”œâ”€â”€ getModels()
  â””â”€â”€ countTokens(text)
```

**Äá»c khi:**
- Cáº§n hiá»ƒu tá»•ng quan vá» kiáº¿n trÃºc LLM module
- Thiáº¿t káº¿ providers má»›i
- Implement core features

---

### 2ï¸âƒ£ [LLM_MULTIPLE_TOOLS_AND_SUMMARY.md](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md)
**Chi tiáº¿t vá» multiple tool calls vÃ  auto-summarization**

**Ná»™i dung chÃ­nh:**
- âœ… **Multiple Tool Calls** - LLM cÃ³ thá»ƒ gá»i nhiá»u tools trong má»™t response
- âœ… **Sequential Execution** - Thá»±c thi tools theo thá»© tá»±
- âœ… **Parallel Execution** - Tá»‘i Æ°u vá»›i dependency analysis
- âœ… **Auto Summarization** - Tá»± Ä‘á»™ng tÃ³m táº¯t khi conversation dÃ i
- âœ… **Custom Summarizer** - TÃ¹y chá»‰nh cÃ¡ch tÃ³m táº¯t
- âœ… **Smart Context Management** - Quáº£n lÃ½ context window thÃ´ng minh

**Tool Execution Strategies:**
- Sequential: Thá»±c thi tá»«ng tool theo thá»© tá»±
- Parallel: PhÃ¢n tÃ­ch dependencies vÃ  cháº¡y song song khi cÃ³ thá»ƒ

**Summarization Strategies:**
- LLMSummarizer: DÃ¹ng LLM Ä‘á»ƒ tÃ³m táº¯t (cháº¥t lÆ°á»£ng cao)
- ExtractiveSummarizer: TrÃ­ch xuáº¥t key points (nhanh, ráº»)
- CustomSummarizer: TÃ¹y chá»‰nh logic riÃªng

**Äá»c khi:**
- Implement function calling vá»›i nhiá»u tools
- Cáº§n quáº£n lÃ½ conversations dÃ i
- Tá»‘i Æ°u token usage

---

### 3ï¸âƒ£ [LLM_FUNCTION_ANNOTATION_DESIGN.md](./LLM_FUNCTION_ANNOTATION_DESIGN.md)
**Annotation-based function definition vá»›i auto-scanning**

**Ná»™i dung chÃ­nh:**
- âœ… **@LLMFunction** - ÄÃ¡nh dáº¥u methods lÃ  LLM-callable
- âœ… **@Param** - MÃ´ táº£ parameters vá»›i validation
- âœ… **@FunctionProvider** - ÄÃ¡nh dáº¥u classes chá»©a functions
- âœ… **Auto Scanning** - Tá»± Ä‘á»™ng quÃ©t vÃ  Ä‘Äƒng kÃ½ functions
- âœ… **DI Integration** - Thá»±c thi trong application context
- âœ… **Reflection-based Execution** - Gá»i methods Ä‘á»™ng

**Example:**
```java
@FunctionProvider
public class SearchFunctions {
    
    @LLMFunction(
        name = "search_projects",
        description = "Search for projects in database"
    )
    public List<Project> searchProjects(
        @Param(description = "Search query", required = true)
        String query,
        
        @Param(description = "Maximum results", defaultValue = "10")
        int limit
    ) {
        // Implementation
        return projectService.search(query, limit);
    }
}
```

**Auto-registration:**
```java
// At startup
FunctionRegistry registry = FunctionRegistry.getInstance();
registry.scanPackage("com.noteflix.pcm.functions");
// All @LLMFunction methods are now registered!
```

**Äá»c khi:**
- Implement custom functions cho LLM
- Setup function scanning
- Integrate vá»›i DI container

---

### 4ï¸âƒ£ [LLM_LOGGING_DESIGN.md](./LLM_LOGGING_DESIGN.md)
**Logging vÃ  audit trail system cho LLM calls**

**Ná»™i dung chÃ­nh:**
- âœ… **Complete Audit Trail** - LÆ°u Ä‘áº§y Ä‘á»§ request/response
- âœ… **Tool Execution Logs** - Log tá»«ng tool call vá»›i params & results
- âœ… **Multiple Storage** - Database (SQLite) hoáº·c File (JSON)
- âœ… **Async Logging** - KhÃ´ng block LLM calls
- âœ… **Queryable** - Search vÃ  filter logs dá»… dÃ ng
- âœ… **Analytics** - Token usage, cost tracking, performance metrics

**Data Models:**
- **LLMCallLog**: ToÃ n bá»™ thÃ´ng tin 1 LLM call
  - Request: messages, options, tools
  - Response: content, thinking, tool calls, usage
  - Metadata: timestamp, duration, user, session
  - Error: cÃ³ lá»—i khÃ´ng, error message

- **ToolCallLog**: Chi tiáº¿t execution cá»§a 1 tool
  - Input: tool name, arguments
  - Output: result hoáº·c error
  - Timing: execution duration

**Storage Options:**
- **DatabaseLogger**: SQLite vá»›i indexes (fast queries)
- **FileLogger**: JSON files organized by date (simple)
- **CompositeLogger**: Log to multiple destinations

**Äá»c khi:**
- Setup logging cho production
- Cáº§n tracking costs
- Debug LLM interactions
- Compliance requirements

---

### 5ï¸âƒ£ [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md)
**Tool result caching vÃ  prompt template system**

**Ná»™i dung chÃ­nh:**

#### **A. Tool Result Caching**
- âœ… **Cache Strategy Pattern** - Linh hoáº¡t chá»n cÃ¡ch xá»­ lÃ½ tool results
- âœ… **Smart Summarization** - Tá»± Ä‘á»™ng quyáº¿t Ä‘á»‹nh full/summary
- âœ… **Token Budget Control** - Kiá»ƒm soÃ¡t chi phÃ­ tokens
- âœ… **Adaptive Learning** - Há»c tá»« usage patterns

**Caching Strategies:**

1. **AlwaysFullStrategy** (Default)
   - LuÃ´n gá»­i full results
   - Accuracy tá»‘i Ä‘a
   - Use case: Khi chÃ­nh xÃ¡c quan trá»ng nháº¥t

2. **SmartSummarizationStrategy**
   - Tá»± Ä‘á»™ng quyáº¿t Ä‘á»‹nh dá»±a trÃªn kÃ­ch thÆ°á»›c
   - Small results: full
   - Large results: summarize
   - Medium: depends on context window
   - Use case: CÃ¢n báº±ng accuracy vs cost

3. **TokenBudgetStrategy**
   - Strict token limit per tool result
   - Use case: Budget constraints

4. **AdaptiveStrategy**
   - Há»c tá»« history
   - Cache & reuse similar results
   - Avoid summarization náº¿u gÃ¢y lá»—i
   - Use case: Production optimization

**Cache Decision Logic:**
```java
CacheDecision {
    shouldCache: boolean        // Cache Ä‘á»ƒ reuse?
    shouldSummarize: boolean    // Summarize trÆ°á»›c khi gá»­i LLM?
    summarizationStrategy: String
    reason: String
}
```

#### **B. Prompt Template System**
- âœ… **Template Registry** - Quáº£n lÃ½ táº­p trung prompts
- âœ… **Variable Substitution** - Dynamic prompt generation
- âœ… **i18n Support** - Multi-language prompts
- âœ… **File-based Loading** - Load tá»« files
- âœ… **Advanced Features** - Conditionals, loops, nested variables

**Built-in Templates:**
- `system.default` - Default system message
- `system.with_role` - System message vá»›i role
- `summarize.conversation` - TÃ³m táº¯t conversation
- `summarize.tool_result` - TÃ³m táº¯t tool result
- `function.instruction` - Function calling instructions
- `thinking.instruction` - Thinking mode instructions
- `error.tool_execution` - Tool error recovery

**Multi-language Example:**
```java
promptRegistry.register("system.helpful", I18nPromptTemplate.builder()
    .templatesByLocale(Map.of(
        Locale.ENGLISH, "You are a helpful AI assistant.",
        Locale.forLanguageTag("vi"), "Báº¡n lÃ  má»™t trá»£ lÃ½ AI há»¯u Ã­ch.",
        Locale.CHINESE, "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"
    ))
    .build());

promptRegistry.setLocale(Locale.forLanguageTag("vi"));
String prompt = promptRegistry.render("system.helpful", Map.of());
// Output: "Báº¡n lÃ  má»™t trá»£ lÃ½ AI há»¯u Ã­ch."
```

**Äá»c khi:**
- Cáº§n tá»‘i Æ°u token costs
- Setup prompt templates
- Implement multi-language support
- Cache tool results

---

### 6ï¸âƒ£ [LLM_MODULE_STRUCTURE.md](./LLM_MODULE_STRUCTURE.md)
**Current structure cá»§a LLM module (existing code)**

**Ná»™i dung chÃ­nh:**
- ğŸ“‚ Package structure hiá»‡n táº¡i
- ğŸ“„ CÃ¡c file quan trá»ng
- ğŸ” Code cáº§n refactor

**Äá»c khi:**
- Cáº§n hiá»ƒu code hiá»‡n táº¡i
- Planning refactoring
- Onboarding new developers

---

## ğŸ¯ Quick Reference

### **TÃ´i muá»‘n...**

| Má»¥c tiÃªu | Äá»c tÃ i liá»‡u | Pháº§n cá»¥ thá»ƒ |
|----------|--------------|-------------|
| Hiá»ƒu tá»•ng quan kiáº¿n trÃºc | [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md) | Â§ New Architecture |
| Setup providers | [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md) | Â§ Provider Registry |
| Implement function calling | [LLM_FUNCTION_ANNOTATION_DESIGN.md](./LLM_FUNCTION_ANNOTATION_DESIGN.md) | ToÃ n bá»™ |
| Handle multiple tool calls | [LLM_MULTIPLE_TOOLS_AND_SUMMARY.md](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md) | Â§ Multiple Tool Calls |
| Setup logging | [LLM_LOGGING_DESIGN.md](./LLM_LOGGING_DESIGN.md) | Â§ Complete Example |
| Tá»‘i Æ°u token costs | [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md) | Â§ Tool Result Caching |
| Customize prompts | [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md) | Â§ Prompt Template System |
| Multi-language prompts | [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md) | Â§ Multi-language Support |
| Auto-summarize conversations | [LLM_MULTIPLE_TOOLS_AND_SUMMARY.md](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md) | Â§ Auto Summarization |

---

## ğŸ“Š Feature Matrix

| Feature | Design Doc | Priority | Status |
|---------|-----------|----------|--------|
| Provider Registry | [Design](./LLM_REFACTOR_DESIGN.md) | ğŸ”´ High | Pending |
| Token Counter | [Design](./LLM_REFACTOR_DESIGN.md) | ğŸ”´ High | Pending |
| Function Registry | [Design](./LLM_REFACTOR_DESIGN.md) | ğŸ”´ High | Pending |
| Event System | [Design](./LLM_REFACTOR_DESIGN.md) | ğŸ”´ High | Pending |
| Multiple Tool Calls | [Design](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md) | ğŸŸ¡ Medium | Pending |
| Auto Summarization | [Design](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md) | ğŸŸ¡ Medium | Pending |
| Annotation Scanning | [Design](./LLM_FUNCTION_ANNOTATION_DESIGN.md) | ğŸŸ¡ Medium | Pending |
| LLM Call Logging | [Design](./LLM_LOGGING_DESIGN.md) | ğŸ”´ High | Pending |
| Tool Result Caching | [Design](./LLM_TOOL_CACHE_AND_PROMPTS.md) | ğŸŸ¡ Medium | Pending |
| Prompt Templates | [Design](./LLM_TOOL_CACHE_AND_PROMPTS.md) | ğŸŸ¢ Low | Pending |

---

## ğŸ—ï¸ Implementation Order

### **Phase 1: Core Infrastructure** ğŸ”´
1. Provider Interface & Registry
2. Token Counter (default + custom)
3. Message models vá»›i System support
4. Event system (ChatEventListener)

**Docs:** [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md)

### **Phase 2: Function Calling** ğŸ”´
1. Standardized Tool format
2. FunctionRegistry basic
3. Annotation scanning
4. DI integration

**Docs:** [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md), [LLM_FUNCTION_ANNOTATION_DESIGN.md](./LLM_FUNCTION_ANNOTATION_DESIGN.md)

### **Phase 3: Providers** ğŸ”´
1. Refactor OpenAI provider
2. Refactor Anthropic provider
3. Refactor Ollama provider
4. Provider capabilities

**Docs:** [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md)

### **Phase 4: Advanced Features** ğŸŸ¡
1. Multiple tool calls support
2. Tool result caching
3. Auto-summarization
4. Thinking mode

**Docs:** [LLM_MULTIPLE_TOOLS_AND_SUMMARY.md](./LLM_MULTIPLE_TOOLS_AND_SUMMARY.md), [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md)

### **Phase 5: Observability** ğŸ”´
1. LLM call logging
2. Tool execution logging
3. Analytics & statistics
4. UI integration

**Docs:** [LLM_LOGGING_DESIGN.md](./LLM_LOGGING_DESIGN.md)

### **Phase 6: Polish** ğŸŸ¢
1. Prompt template system
2. i18n prompts
3. Error handling improvements
4. Documentation & examples

**Docs:** [LLM_TOOL_CACHE_AND_PROMPTS.md](./LLM_TOOL_CACHE_AND_PROMPTS.md)

---

## ğŸ”‘ Key Design Decisions

### **1. Why Provider Registry Pattern?**
- âœ… Easy to switch between providers
- âœ… Centralized configuration
- âœ… Consistent API across providers
- âœ… Support multiple active providers

### **2. Why Annotation-based Functions?**
- âœ… Declarative & clean
- âœ… Auto-discovery
- âœ… Type-safe with validation
- âœ… Easy to maintain

### **3. Why Cache Tool Results?**
- âœ… Avoid redundant expensive operations
- âœ… Control token costs
- âœ… Improve response time
- âœ… Flexible strategies per use case

### **4. Why Prompt Templates?**
- âœ… Separate prompts from code
- âœ… Easy A/B testing
- âœ… Multi-language support
- âœ… Version control friendly

### **5. Why Comprehensive Logging?**
- âœ… Debugging LLM interactions
- âœ… Cost tracking & optimization
- âœ… Audit trail for compliance
- âœ… Performance monitoring

---

## ğŸ“– How to Read These Docs

### **For Architects/Tech Leads:**
1. Start with [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md) - Overall architecture
2. Review all Â§ Architecture sections
3. Evaluate design decisions
4. Plan implementation phases

### **For Developers (Implementing):**
1. Read relevant spec for your feature
2. Follow code examples
3. Check Â§ Integration sections
4. Refer to Â§ File Structure for where to put code

### **For New Team Members:**
1. Start with [LLM_MODULE_STRUCTURE.md](./LLM_MODULE_STRUCTURE.md) - Current state
2. Read [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md) - Future state
3. Skim other docs to understand capabilities
4. Deep dive when implementing specific features

---

## ğŸ¨ Design Principles

All specifications follow these principles:

1. **Simple by Default** - Easy to use for common cases
2. **Flexible for Advanced** - Powerful features available when needed
3. **Provider Agnostic** - Work with any LLM provider
4. **Type Safe** - Leverage Java type system
5. **Observable** - Log everything, measure everything
6. **Maintainable** - Clean architecture, clear separation
7. **Testable** - All components are unit testable

---

## ğŸš€ Getting Started

### **To Implement a New Feature:**

1. **Read the spec** - Find relevant document above
2. **Understand the design** - Review architecture diagrams
3. **Check dependencies** - See implementation order
4. **Write tests first** - Test-driven development
5. **Implement** - Follow code examples in specs
6. **Integrate** - Connect with existing components
7. **Document** - Update examples and README

### **To Propose Changes:**

1. **Identify the issue** - What's wrong with current design?
2. **Suggest solution** - How to improve?
3. **Update spec** - Modify relevant document
4. **Discuss trade-offs** - What are pros/cons?
5. **Get consensus** - Team agreement
6. **Implement** - Make it happen!

---

## ğŸ“ Questions?

**Architecture Questions:**
- Review [LLM_REFACTOR_DESIGN.md](./LLM_REFACTOR_DESIGN.md) first
- Check design principles section
- Discuss with tech lead

**Implementation Questions:**
- Find the relevant spec
- Check code examples
- Refer to Â§ Integration sections

**Missing Information:**
- Document is incomplete
- Open an issue or discussion
- Propose additions to specs

---

## ğŸ“ Document Status

| Document | Version | Last Updated | Status |
|----------|---------|--------------|--------|
| LLM_REFACTOR_DESIGN.md | 1.0 | 2025-11-12 | âœ… Complete |
| LLM_MULTIPLE_TOOLS_AND_SUMMARY.md | 1.0 | 2025-11-12 | âœ… Complete |
| LLM_FUNCTION_ANNOTATION_DESIGN.md | 1.0 | 2025-11-12 | âœ… Complete |
| LLM_LOGGING_DESIGN.md | 1.0 | 2025-11-12 | âœ… Complete |
| LLM_TOOL_CACHE_AND_PROMPTS.md | 1.0 | 2025-11-12 | âœ… Complete |
| README.md (this) | 1.0 | 2025-11-12 | âœ… Complete |

---

**All designs are complete and ready for implementation!** ğŸ‰

Next step: Start Phase 1 implementation (Core Infrastructure)


# ğŸ¤– LLM Integration - PCM Desktop

## ğŸ“‹ Overview

Há»‡ thá»‘ng tÃ­ch há»£p LLM linh hoáº¡t, extensible, tuÃ¢n thá»§ SOLID principles vÃ  clean code practices.

### âœ¨ Key Features

- âœ… **Multiple Providers**: OpenAI, Anthropic, Ollama, Custom APIs
- âœ… **Streaming Support**: Real-time response streaming (SSE)
- âœ… **Function Calling**: Tool/function execution
- âœ… **Flexible Configuration**: URL + Token per provider
- âœ… **Easy Extension**: Add new provider in < 2 hours
- âœ… **Type-safe**: Strong typing throughout
- âœ… **Clean Architecture**: SOLID principles applied

---

## ğŸ“š Documentation

### Main Documents

1. **[LLM_INTEGRATION_PLAN.md](./development/LLM_INTEGRATION_PLAN.md)** - Complete Implementation Plan
    - Architecture design
    - SOLID principles explained
    - Design patterns
    - 6-week implementation phases
    - Provider examples
    - Testing strategy

2. **[LLM_QUICK_START.md](./development/LLM_QUICK_START.md)** - Quick Start Guide
    - Core concepts
    - Usage examples
    - Adding new providers
    - Configuration examples
    - Common issues

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      UI Layer (JavaFX)          â”‚
â”‚  Chat UI, Streaming Display     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Service Layer (Business)     â”‚
â”‚  LLMService, ConversationServiceâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client Layer (Abstraction)     â”‚
â”‚  LLMClient Interface            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
â”‚OpenAI â”‚  â”‚ Anthropic  â”‚  â”‚Custom â”‚
â”‚Client â”‚  â”‚  Client    â”‚  â”‚Client â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Package Structure

```
com.noteflix.pcm.llm/
â”œâ”€â”€ api/                    # âœ… Interfaces
â”‚   â”œâ”€â”€ LLMClient.java
â”‚   â”œâ”€â”€ StreamingCapable.java
â”‚   â””â”€â”€ FunctionCallingCapable.java
â”‚
â”œâ”€â”€ model/                  # âœ… Data models  
â”‚   â”œâ”€â”€ LLMRequest.java
â”‚   â”œâ”€â”€ LLMResponse.java
â”‚   â”œâ”€â”€ Message.java
â”‚   â”œâ”€â”€ LLMChunk.java
â”‚   â”œâ”€â”€ FunctionDefinition.java
â”‚   â”œâ”€â”€ FunctionCall.java
â”‚   â”œâ”€â”€ StreamingObserver.java
â”‚   â””â”€â”€ LLMProviderConfig.java
â”‚
â”œâ”€â”€ client/                 # â³ Provider implementations
â”‚   â”œâ”€â”€ openai/
â”‚   â”œâ”€â”€ anthropic/
â”‚   â”œâ”€â”€ ollama/
â”‚   â””â”€â”€ custom/
â”‚
â”œâ”€â”€ factory/                # â³ Factory
â”‚   â””â”€â”€ LLMClientFactory.java
â”‚
â””â”€â”€ service/                # â³ Services
    â””â”€â”€ LLMService.java
```

---

## ğŸ’» Quick Example

### Basic Chat

```java
// Configure provider
LLMProviderConfig config = LLMProviderConfig.builder()
    .provider(Provider.OPENAI)
    .url("https://api.openai.com/v1/chat/completions")
    .token(System.getenv("OPENAI_API_KEY"))
    .model("gpt-4")
    .build();

// Create client
LLMClient client = LLMClientFactory.createClient(config);

// Send message
LLMRequest request = LLMRequest.builder()
    .model("gpt-4")
    .messages(List.of(Message.user("What is Java?")))
    .build();

LLMResponse response = client.sendMessage(request);
System.out.println(response.getContent());
```

### Streaming Chat

```java
if (client instanceof StreamingCapable streamingClient) {
    streamingClient.streamMessage(request, new StreamingObserver() {
        @Override
        public void onChunk(LLMChunk chunk) {
            System.out.print(chunk.getContent());
        }
        
        @Override
        public void onComplete() {
            System.out.println("\nDone!");
        }
        
        @Override
        public void onError(Throwable error) {
            error.printStackTrace();
        }
    });
}
```

### Function Calling

```java
FunctionDefinition func = FunctionDefinition.builder()
    .name("get_weather")
    .description("Get current weather")
    .parameters(createJsonSchema())
    .build();

if (client instanceof FunctionCallingCapable funcClient) {
    LLMResponse response = funcClient.sendWithFunctions(
        request, 
        List.of(func)
    );
    
    if (response.hasFunctionCall()) {
        // Execute function
        Object result = executeFunction(response.getFunctionCall());
        // Send result back
    }
}
```

---

## âœ… SOLID Principles

### Single Responsibility

```java
LLMClient        â†’ Send/receive messages
StreamingCapable â†’ Handle streaming
FunctionExecutor â†’ Execute functions
```

### Open/Closed

```java
// Add new provider WITHOUT modifying existing code
public class NewProvider implements LLMClient { }
```

### Liskov Substitution

```java
// Any LLMClient implementation works
LLMClient client = new AnyProvider(config);
```

### Interface Segregation

```java
// Implement only what you support
public class SimpleClient implements LLMClient { }
public class FullClient implements LLMClient, StreamingCapable, FunctionCallingCapable { }
```

### Dependency Inversion

```java
// Depend on abstraction
public class Service {
    private final LLMClient client; // Interface
}
```

---

## ğŸ¨ Design Patterns

| Pattern                     | Purpose              | Benefit                    |
|-----------------------------|----------------------|----------------------------|
| **Strategy**                | Provider selection   | Easy provider switching    |
| **Factory**                 | Client creation      | Centralized creation logic |
| **Builder**                 | Request construction | Flexible object building   |
| **Observer**                | Streaming            | Reactive updates           |
| **Adapter**                 | Format conversion    | Uniform interface          |
| **Chain of Responsibility** | Middleware           | Extensible processing      |

---

## ğŸš€ Implementation Status

### Phase 1: Foundation âœ… COMPLETE

- [x] Core interfaces (LLMClient, StreamingCapable, FunctionCallingCapable)
- [x] Model classes (LLMRequest, LLMResponse, Message, etc.)
- [x] Configuration model
- [x] Documentation

### Phase 2: OpenAI Client â³ TODO

- [ ] OpenAIClient implementation
- [ ] Basic chat support
- [ ] Streaming support
- [ ] Function calling support
- [ ] HTTP client wrapper
- [ ] Error handling
- [ ] Unit tests

### Phase 3: Additional Providers â³ TODO

- [ ] AnthropicClient (Claude)
- [ ] OllamaClient (Local)
- [ ] CustomClient (Generic)
- [ ] Provider adapters
- [ ] Tests

### Phase 4: Factory & Service â³ TODO

- [ ] LLMClientFactory
- [ ] LLMService
- [ ] ConversationService
- [ ] FunctionExecutorService
- [ ] Middleware chain

### Phase 5: UI Integration â³ TODO

- [ ] AIAssistantPage integration
- [ ] Streaming UI
- [ ] Function calling UI
- [ ] Provider selection
- [ ] Configuration UI

### Phase 6: Advanced Features â³ TODO

- [ ] Response caching
- [ ] Rate limiting
- [ ] Retry logic
- [ ] Token counting
- [ ] Conversation persistence

---

## ğŸ”§ Supported Providers

| Provider      | Status    | Streaming | Function Calling | Config      |
|---------------|-----------|-----------|------------------|-------------|
| **OpenAI**    | â³ Planned | âœ… Yes     | âœ… Yes            | URL + Token |
| **Anthropic** | â³ Planned | âœ… Yes     | âœ… Yes            | URL + Token |
| **Ollama**    | â³ Planned | âœ… Yes     | âŒ No             | URL only    |
| **Custom**    | â³ Planned | âš ï¸ Maybe  | âš ï¸ Maybe         | URL + Token |

---

## ğŸ“Š Configuration Examples

### OpenAI

```java
LLMProviderConfig.builder()
    .provider(Provider.OPENAI)
    .url("https://api.openai.com/v1/chat/completions")
    .token(System.getenv("OPENAI_API_KEY"))
    .model("gpt-4")
    .supportsStreaming(true)
    .supportsFunctionCalling(true)
    .build();
```

### Anthropic (Claude)

```java
LLMProviderConfig.builder()
    .provider(Provider.ANTHROPIC)
    .url("https://api.anthropic.com/v1/messages")
    .token(System.getenv("ANTHROPIC_API_KEY"))
    .model("claude-3-5-sonnet-20241022")
    .supportsStreaming(true)
    .supportsFunctionCalling(true)
    .build();
```

### Ollama (Local)

```java
LLMProviderConfig.builder()
    .provider(Provider.OLLAMA)
    .url("http://localhost:11434/api/chat")
    .token("") // No token
    .model("llama3")
    .supportsStreaming(true)
    .supportsFunctionCalling(false)
    .build();
```

---

## ğŸ¯ Success Metrics

| Metric           | Target    | Status            |
|------------------|-----------|-------------------|
| Add new provider | < 2 hours | âœ… Design complete |
| Response time    | < 3s      | ğŸ¯ To measure     |
| Test coverage    | > 80%     | ğŸ“Š To achieve     |
| Code quality     | A rating  | ğŸ“ˆ To achieve     |
| Extensibility    | Easy      | âœ… SOLID applied   |

---

## ğŸ“– Learn More

### Core Concepts

- **LLMClient**: Base interface for all providers
- **StreamingCapable**: Optional streaming support
- **FunctionCallingCapable**: Optional function calling
- **LLMRequest/Response**: Universal message format
- **LLMProviderConfig**: Provider configuration

### Best Practices

- Use interfaces for flexibility
- Implement only what you need (ISP)
- Depend on abstractions (DIP)
- Builder pattern for complex objects
- Observer pattern for streaming

---

## ğŸ†˜ Support

### Get Help

- Read [LLM_INTEGRATION_PLAN.md](./development/LLM_INTEGRATION_PLAN.md) for details
- Read [LLM_QUICK_START.md](./development/LLM_QUICK_START.md) for examples
- Check code documentation (JavaDoc)

### Resources

- [OpenAI API Docs](https://platform.openai.com/docs/api-reference)
- [Anthropic API Docs](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)
- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)

---

## ğŸ“ Next Steps

1. âœ… **Review** the implementation plan
2. âœ… **Understand** SOLID principles applied
3. â³ **Start Phase 2**: Implement OpenAIClient
4. â³ **Write tests** for all components
5. â³ **Add more providers** as needed
6. â³ **Integrate with UI** for user-facing features

---

**Status**: âœ… Foundation Complete - Ready for Implementation  
**Created**: November 12, 2025  
**Last Updated**: November 12, 2025  
**Version**: 1.0.0

---

**Â© 2025 PCM Desktop - Noteflix Team**

